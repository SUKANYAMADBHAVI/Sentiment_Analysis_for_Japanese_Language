# Sentiment-Analysis-for-Japanese
This project aimed to develop a sentiment classifier for the Japanese language using two different models: BERT and LSTM. The dataset, obtained from Kaggle, was converted from JSON to CSV format, and specialized datasets for stopwords and sentiment words in Japanese were created. Data preprocessing techniques, including stopword removal, tokenization, part-of-speech tagging, and lemma extraction, were applied to improve the dataset quality.The BERT-based model achieved an accuracy of 79%, surpassing the LSTM-based model's accuracy of 56%. The superior performance of BERT can be attributed to its fine-tuning on the sentiment dataset and its ability to capture deep contextual information. To facilitate its utilization, the BERT model was integrated with Flask, allowing the sentiment classifier to be deployed as an API.The data preprocessing steps involved converting the dataset to CSV format, removing stopwords, tokenizing the text, performing part-of-speech tagging, and extracting lemmas. These steps aimed to enhance the dataset quality by reducing noise and ensuring consistency. Specialized datasets for Japanese stopwords and sentiment words were created, further improving sentiment classification accuracy.The sentiment classifier for Japanese text achieved an accuracy of 79% using BERT, indicating its ability to capture contextual information and accurately predict sentiment. The LSTM-based model achieved 56% accuracy but has potential for improvement through fine-tuning and hyperparameter optimization.This project created a successful sentiment classifier for Japanese text using BERT as the frontend model. By employing data preprocessing techniques, the classifier accurately predicted sentiment labels. It can be easily integrated into web applications through its API deployment, allowing valuable insights to be extracted from Japanese text data. Future work includes improving the LSTM model's performance and adapting the classifier for other languages.
